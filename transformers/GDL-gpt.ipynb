{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ceeee90-6c94-49ba-b060-85cf304f2205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dirF:\\ml-from-scratch\n"
     ]
    }
   ],
   "source": [
    "exec(open('init_notebook.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b8c3e8-458d-4172-8269-9a3255a0318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68d8a9-37ea-40f1-ad18-546dca6dd961",
   "metadata": {},
   "source": [
    "# Resources\n",
    "1. Generative Deep Learning by David Foster\n",
    "2. https://github.com/karpathy/minGPT/blob/master/demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75e886-fb06-4a1e-9964-c73d79f78de1",
   "metadata": {},
   "source": [
    "GPT is a **decoder** only model (pretraining model to be used by different decoders). We will build a casual masked language model (Casual MLM)\n",
    "\n",
    "# PipeLine\n",
    "1. Preprocessing Text -> Tokenize -> Sentence Clipper/Padder -> Input Dataset\n",
    "2. Input Dataset -> token embedding + positional embedding -> input embedding\n",
    "3. Input Sequence Embeddings -> self-attention A(Q,K,V) -> single head\n",
    "4. Single Heads -> concat attentions -> Multihead\n",
    "5. Training\n",
    "   \n",
    "\n",
    "## Encoder\n",
    "5. multi-head + query -> layer-norm -> FFNs -> layer-norm -> output next token\n",
    "\n",
    "In GPT, both token and positional embeddings are learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ce282-3444-48fb-a609-95cf430cdc81",
   "metadata": {},
   "source": [
    "**This is the GPT implementation in Pytorch. The book has the tensorflow+keras implementation. Borrowed some ideas from minGPT**\n",
    "### Dependencies:\n",
    "1. Pytorch 2.x (should work in 1.x, too, as we are using as less as possible from torch)\n",
    "2. Pandas\n",
    "3. scikit-learn\n",
    "4. tqdm\n",
    "5. numpy\n",
    "6. matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "817f9a00-3a44-4e20-9573-d78665d58a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "from typing import *\n",
    "import numpy.typing as npt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import heapq\n",
    "\n",
    "# Custom types\n",
    "Index = int\n",
    "Char = str\n",
    "Path = str\n",
    "Sentence = str\n",
    "TokenizedSentence = List[int]\n",
    "Word = str\n",
    "TokenId = int\n",
    "Corpus = List[Sentence]\n",
    "TokenizedCorpus = List[TokenizedSentence]\n",
    "Vocabulary = Dict[Word, TokenId]\n",
    "\n",
    "# Torch types, we use numpy typing\n",
    "SequenceLength = int\n",
    "EmbeddingDim = int\n",
    "Vector = npt.NDArray \n",
    "Embedding = Vector\n",
    "SequenceEmbedding = npt.NDArray[Vector] # [seqLen, embeddingDim]\n",
    "BatchSequenceEmbedding = npt.NDArray[SequenceEmbedding] # [batch, seqlen, embeddingDim]\n",
    "\n",
    "# Attention types\n",
    "Query = Vector\n",
    "Queries = npt.NDArray[Query]\n",
    "BatchQueries = npt.NDArray[Queries]\n",
    "Key = Vector\n",
    "Keys = npt.NDArray[Key]\n",
    "BatchKeys = npt.NDArray[Keys]\n",
    "Value = Vector\n",
    "Values = npt.NDArray[Value]\n",
    "BatchValues = npt.NDArray[Values]\n",
    "AttentionMask = npt.NDArray[SequenceEmbedding] # [batch, seqlen, embeddingDim]\n",
    "\n",
    "Attention = Value # a single weighted value embedding for a query\n",
    "Attentions = npt.NDArray[Attention] # for all the queries of a sentence\n",
    "BatchAttentions = npt.NDArray[Attentions] # for a batch of sentences, batch, seqlen, seqlen\n",
    "\n",
    "AttentionLogit = Vector # each query has d_k attention scores. these are logits, not softmax weights\n",
    "AttentionLogits = npt.NDArray[AttentionLogit]\n",
    "BatchAttentionLogits = npt.NDArray[AttentionLogits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff028ae5-595d-4768-894d-bb6ec280ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "dataDir = \"E:/Datasets/wine-reviews\"\n",
    "vocabSize = 10_000\n",
    "sentenceLength = 100\n",
    "embeddingDim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458a95b1-645a-4959-a86f-b2c46855221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "############# The Pipeline class which is the orchestration of each step of the process ###############\n",
    "#######################################################################################################\n",
    "class Pipeline:\n",
    "    def __init__(self, vocabSize: int, sentenceLength: int, embeddingDim: int):\n",
    "        self.vocabSize = vocabSize\n",
    "        self.sentenceLength = sentenceLength\n",
    "        self.embeddingDim = embeddingDim\n",
    "        pass\n",
    "        \n",
    "    def save(self, directory: Path):\n",
    "        pass\n",
    "        \n",
    "    def preprocess(self, corpus: Corpus) -> Corpus:\n",
    "        pass\n",
    "    def tokenize(self, corpus: Corpus, vocabSize: int) -> Tuple[Vocabulary, TokenizedCorpus]:\n",
    "        pass\n",
    "    def createDatasets(self, tokenizedCorpus: TokenizedCorpus, sentenceLength) -> Tuple[Dataset, Dataset]:\n",
    "        pass\n",
    "    \n",
    "pipeline = Pipeline(vocabSize, sentenceLength, embeddingDim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe1634-a975-44b1-9d7c-9bf8cc1d8e44",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing\n",
    "1. read text,\n",
    "2. pad punctuations with spaces to convert them to individual words.                                                                                                                                                                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d82e07b-8ca5-401d-aabc-58dc4ac99729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dataDir, \"winemag-data-130k-v2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32dd067d-1759-4d0d-9851-d7cd2b8eb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e8f1a4-278a-459c-b87e-5b100323a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"description\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bab086c-047e-4b33-b51d-a0ca5c39a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa0f1ec-afb1-483b-8450-f15ddadeb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessorText:\n",
    "    \n",
    "    def padPunk(self, s: Sentence) -> Sentence: # this does not work well. We will do BPE next. isn't -> becomes bad is ' t. now t is a word! It's it ' s\n",
    "        s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s) #to words\n",
    "        s = re.sub(' +', ' ', s) # multiple spaces to one\n",
    "        return s\n",
    "    \n",
    "    def smallCase(self, corpus: List[Sentence]) -> List[Sentence]:\n",
    "        return [s.lower() for s in corpus]\n",
    "\n",
    "    def __call__(self, corpus: List[Sentence]) -> List[Sentence]:\n",
    "        corpus = [self.padPunk(s) for s in corpus] \n",
    "        return self.smallCase(corpus)\n",
    "        \n",
    "pipeline.preprocess = lambda corpus: PreProcessorText()(corpus)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e84cabff-ecc4-4dcd-9f8e-06cb3585509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pipeline.preprocess(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f2f1306-bd8b-46c2-9ca8-4f4ca0ab821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"aromas include tropical fruit , broom , brimstone and dried herb . the palate isn ' t overly expressive , offering unripened apple , citrus and dried sage alongside brisk acidity . \",\n",
       " \"this is ripe and fruity , a wine that is smooth while still structured . firm tannins are filled out with juicy red berry fruits and freshened with acidity . it ' s already drinkable , although it will certainly be better from 2016 . \",\n",
       " 'tart and snappy , the flavors of lime flesh and rind dominate . some green pineapple pokes through , with crisp acidity underscoring the flavors . the wine was all stainless - steel fermented . ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb9841-70b8-41e8-a09d-423710ceaf6e",
   "metadata": {},
   "source": [
    "## 1.2 Tokenize\n",
    "1. vocabulary\n",
    "2. tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe3db34a-0177-4972-9995-60b65ab3fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTokenizer:\n",
    "    \n",
    "    def getVocabulary(self, corpus: Corpus, vocabSize: int) -> Vocabulary:\n",
    "        vectorizer = CountVectorizer().fit(corpus)\n",
    "        topWords = heapq.nlargest(vocabSize-1, vectorizer.vocabulary_, key=lambda w: vectorizer.vocabulary_[w])\n",
    "        wordToToken = {}\n",
    "        for idx, w in enumerate(topWords):\n",
    "            wordToToken[w] = idx\n",
    "    \n",
    "        wordToToken['UNK'] = len(wordToToken)\n",
    "        print(f\"Created a vocabulary with top {len(wordToToken)} words from {len(vectorizer.vocabulary_)} words with UNK as the last word\")\n",
    "        return wordToToken\n",
    "\n",
    "    def tokenizeSentence(self, sentence: Sentence, vocabulary: Vocabulary) -> TokenizedSentence:\n",
    "        words = sentence.split()\n",
    "        tSen = [\n",
    "            vocabulary[w] if w in vocabulary\n",
    "            else vocabulary[\"UNK\"]\n",
    "                for w in words\n",
    "        ]\n",
    "        return tSen\n",
    "\n",
    "    def build(self, corpus: Corpus, vocabSize: int) -> Tuple[Vocabulary, TokenizedCorpus]:\n",
    "        vocabulary = self.getVocabulary(corpus, vocabSize)\n",
    "        tokenizedCorpus = [self.tokenizeSentence(s, vocabulary) for s in corpus]\n",
    "        return vocabulary, tokenizedCorpus\n",
    "\n",
    "    def __call__(self, corpus: Corpus, vocabSize: int) -> Tuple[Vocabulary, TokenizedCorpus]:\n",
    "        return self.build(corpus, vocabSize)\n",
    "\n",
    "pipeline.tokenize = lambda corpus: SentenceTokenizer()(corpus, pipeline.vocabSize)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd5e84f-96dd-4cb7-b5be-71ab84b115b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a vocabulary with top 10000 words from 31274 words with UNK as the last word\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3415"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, tokenizedCorpus = pipeline.tokenize(corpus)\n",
    "# sentenceTokenizer = SentenceTokenizer()\n",
    "# vocabulary, tokenizedCorpus = sentenceTokenizer.build(corpus, vocabSize)\n",
    "vocabulary[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3970c0d3-7d13-45f8-b640-43c4eefb71cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9999, 9999, 2608, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 3415, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 1901, 9999, 9999, 9999, 9999, 9999, 7486, 9999, 9999, 9999, 9999]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizedCorpus[0]) # mostl UNKs as we have over 30 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552189f9-22cf-4ce8-942b-8387490dbda6",
   "metadata": {},
   "source": [
    "## 1.3 Dataset\n",
    "1. fixed length sentences\n",
    "2. torch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bc8efcc-1578-4b16-b8c3-4ac226007fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, tokenizedCorpus: TokenizedCorpus, split: str, length: int, vocabSize: int):\n",
    "        assert split in {'train', 'test'} # borrowed from minGPT\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "        self.vocabSize = vocabSize\n",
    "        self.corpus = tokenizedCorpus.copy()\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.size = int(len(self.corpus) * 0.7)\n",
    "        else:\n",
    "            self.size = len(self.corpus) - int(len(self.corpus) * 0.7)\n",
    "            \n",
    "        self.ids = self._generateIds()\n",
    "        self._reshapeCorpus()\n",
    "        \n",
    "\n",
    "    def _reshapeCorpus(self):\n",
    "        # converts to fixed length sentences by clipping or padding\n",
    "        for idx in tqdm(self.ids, desc=f\"reshaping {self.split} corpus\"):\n",
    "            self.corpus[idx] = self._reshapeSentence(self.corpus[idx])\n",
    "        pass\n",
    "\n",
    "    def _reshapeSentence(self, sentence: TokenizedSentence) -> TokenizedSentence:\n",
    "        if len(sentence) >= self.length:\n",
    "            return sentence[:self.length]\n",
    "        # we pad with zeros. # zero not in vocab\n",
    "        return sentence + [0] * (self.length - len(sentence))\n",
    "        \n",
    "\n",
    "    def _generateIds(self) -> List[Index]:\n",
    "        # we just get the top for train and bot for test.\n",
    "        if self.split == 'train':\n",
    "            return list(range(self.size))\n",
    "        else:\n",
    "            start = int(len(self.corpus) * 0.7) # train index ends before start\n",
    "            return [i+start for i in range(self.size)]\n",
    "\n",
    "    def getVocabSize(self) -> int:\n",
    "        return self.vocabSize\n",
    "\n",
    "    def getBlockSize(self) -> int:\n",
    "        # as this is an encoder. we feed n inputs, n-1 outputs, and then read outputs? need more clarifications.\n",
    "        return self.vocabSize * 2 - 1\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx: Index) -> TokenizedSentence:\n",
    "        return self.corpus[self.ids[idx]] # ids can be sparse\n",
    "\n",
    "pipeline.createDatasets = lambda tokenizedCorpus: (SentenceDataset(tokenizedCorpus, \"train\", pipeline.sentenceLength, pipeline.vocabSize), SentenceDataset(tokenizedCorpus, \"test\", pipeline.sentenceLength, pipeline.vocabSize))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0f5870a-bd08-427a-9308-c34456521c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reshaping train corpus: 100%|████████████████████████████████████████████████| 90979/90979 [00:00<00:00, 295381.48it/s]\n",
      "reshaping test corpus: 100%|█████████████████████████████████████████████████| 38992/38992 [00:00<00:00, 158501.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# trainSet = SentenceDataset(tokenizedCorpus, \"train\", 100)\n",
    "# testSet = SentenceDataset(tokenizedCorpus, \"test\", 100)\n",
    "trainSet, testSet = pipeline.createDatasets(tokenizedCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06863c93-77f9-4e55-860e-15a56c5b2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert trainSet.ids[-1] + 1 == testSet.ids[0]\n",
    "assert testSet.ids[-1] + 1 == len(tokenizedCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1cbf96-9386-4708-a7c3-499c9d4bd322",
   "metadata": {},
   "source": [
    "## 2. Embedding\n",
    "We will learn both token embedding and positional encoding. However, we can also use pretrained embedding and trigonometric functions (sine, cosine) from the original Transformer for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a383972-e289-4c74-bcfb-70f76a5a5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenEmbedding = nn.Embedding(vocabSize, embeddingDim)\n",
    "# posEmbedding = nn.Embedding(sentenceLength, embeddingDim) # number of positions. the dim is the same as tokens as they will be summed\n",
    "pipeline.createEmbedding = lambda : (nn.Embedding(pipeline.vocabSize, pipeline.embeddingDim), nn.Embedding(pipeline.sentenceLength, pipeline.embeddingDim))\n",
    "tokenEmbedding, posEmbedding = pipeline.createEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5371eb35-e34f-4f1e-93e6-5fec2f739407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(10000, 50), Embedding(100, 50))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenEmbedding, posEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ff114-5449-4fa0-b900-9da3d0863a4c",
   "metadata": {},
   "source": [
    "## 3. The Single Head\n",
    "**From this point on, everything will have a batch dimension at 0**\n",
    "1. Input Embedding\n",
    "2. Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4501d0-c960-4b83-b43c-71e099bb5b8c",
   "metadata": {},
   "source": [
    "### 3.1 Input Embedding\n",
    "1. Convert the sentence token batch to embedding batch\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a70513df-01e6-4cfb-b1d2-5cd28b52a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good\n"
     ]
    }
   ],
   "source": [
    "class InputProcessor:\n",
    "    def embed(self, batch: List[TokenizedSentence], tokenEmbedding: nn.Embedding, posEmbedding: nn.Embedding) -> BatchSequenceEmbedding:\n",
    "        sentenceEmbeddings = []\n",
    "        for sentence in batch:\n",
    "            sentenceEmbeddings.append(self.embedSentence(sentence, tokenEmbedding, posEmbedding))\n",
    "        \n",
    "        return torch.stack(sentenceEmbeddings)\n",
    "\n",
    "    def embedSentence(self, sentenceTokens:TokenizedSentence, tokenEmbedding: nn.Embedding, posEmbedding: nn.Embedding) -> SequenceEmbedding:\n",
    "        positions = torch.tensor(range(len(sentenceTokens)))\n",
    "        posEmbeddings = posEmbedding(positions)\n",
    "        tokenEmbeddings = tokenEmbedding(torch.tensor(sentenceTokens))\n",
    "        # return tokenEmbeddings * math.sqrt(len(sentenceTokens)) + posEmbeddings # we just add a weight to the token embeddings.\n",
    "        return tokenEmbeddings + posEmbeddings # we just add no weight to the token embeddings.\n",
    "\n",
    "\n",
    "def testInputProcessor():\n",
    "    class Embedding:\n",
    "        def __call__(self, indices: Iterable[int]) -> SequenceEmbedding:\n",
    "            embeddings = []\n",
    "            for i in indices:\n",
    "                embeddings.append([i] * 3)\n",
    "            return torch.tensor(embeddings)\n",
    "            \n",
    "    embedding = Embedding() # everything is a index repeated 3 times\n",
    "    inputProcessor = InputProcessor()\n",
    "    batch = [\n",
    "                [1, 2],\n",
    "                [3, 4]\n",
    "    ]\n",
    "\n",
    "    gotEmbeddings = inputProcessor.embed(batch, embedding, embedding)\n",
    "    expectedEmbeddings = torch.tensor(\n",
    "        [\n",
    "            [\n",
    "                [1, 1, 1],\n",
    "                [3, 3, 3] # pos 1 adds 1\n",
    "            ],\n",
    "            [\n",
    "                [3, 3, 3],\n",
    "                [5, 5, 5] # pos 1 adds 1\n",
    "            ]\n",
    "        ]\n",
    "    ) \n",
    "    # print(\"got\", gotEmbeddings)\n",
    "    # print(\"expected\", expectedEmbeddings)\n",
    "    assert np.allclose(gotEmbeddings, expectedEmbeddings)\n",
    "    print(\"All good\")\n",
    "\n",
    "testInputProcessor()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351aa61-2534-4916-a36d-c681367a30dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7dd5caf-85e7-4106-8335-8aca405a9d1b",
   "metadata": {},
   "source": [
    "## 3.2 Self-Attention - Test Driven\n",
    "1. Mask is added after attention scores are computed to save computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b1786ff6-2835-43b4-80a5-f50c2b20bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CasualSingleHead(nn.Module):\n",
    "    def __init__(self, d_tokenEmbedding: int, hiddenSize: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_tokenEmbedding = d_tokenEmbedding\n",
    "        self.hiddenSize = hiddenSize # == d_q, d_k, d_v\n",
    "        self.d_k = self.hiddenSize\n",
    "        # stacked ffns for Q, K, V projections. Or we could use three different linear layers which will be executed sequentially. but this may have issues for batch inputs to a linear layer.\n",
    "        self.qkvProjections = nn.Linear(d_tokenEmbedding, 3 * hiddenSize) # [0- hiddenSize] for q, [hiddenSize - 2* hiddenSize] for k, [2* hiddenSize - 3 * hiddenSize] for v.\n",
    "        self.attentionDropout = nn.Dropout(0.2)\n",
    "        self.residualDropout = nn.Dropout(0.2)\n",
    "        self.skipLayerNormFirst = nn.LayerNorm(self.hiddenSize, eps=1e-6)\n",
    "        self.skipLayerNormLast = nn.LayerNorm(self.hiddenSize, eps=1e-6)\n",
    "        self.ffn1 = nn.Linear(self.hiddenSize, self.hiddenSize * 2)\n",
    "        self.ffn2 = nn.Linear(self.hiddenSize * 2, self.hiddenSize)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, batch: BatchSequenceEmbedding, mask: Optional[AttentionMask]) -> Tuple[BatchAttentions, BatchAttentionLogits]:\n",
    "        # Steps\n",
    "        # 1. input -> Q, K, V\n",
    "        # 2. Q, K, V -> Attention\n",
    "        # 3. optional Contact heads -> project on Q\n",
    "        # 3. skip connection with Q\n",
    "        # 4. layer norm\n",
    "        # 5. FFNs\n",
    "        # 6. Skip connection with previous norm\n",
    "        # 7. layer norm\n",
    "        batchSize, seqLen, tokenEmbeddingSize = batch.size()\n",
    "        batchQueries, batchKeys, batchValues = self.getBatchQKV(batch) \n",
    "        attention, attentionLogits = self.scaledDotProduct(batchQueries, batchKeys, batchValues, mask)\n",
    "        attention  = self.attentionDropout(attention)\n",
    "        \n",
    "        # 3. skip connection with Q\n",
    "        attention += batchQueries \n",
    "        # 4. layer norm\n",
    "        norm1 = self.skipLayerNormFirst(attention)\n",
    "        \n",
    "        # 5. FFNs on norm1\n",
    "        attention = self.ffn1(norm1)\n",
    "        attention = self.ffn2(attention)\n",
    "        \n",
    "        # 6. Skip connection with previous norm\n",
    "        attention += norm1\n",
    "        # 7. layer norm\n",
    "        norm2 = self.skipLayerNormLast(attention)\n",
    "\n",
    "        return norm2, attentionLogits\n",
    "        \n",
    "\n",
    "    def getBatchQKV(self, batch: BatchSequenceEmbedding) -> Tuple[BatchQueries, BatchKeys, BatchValues]:\n",
    "         # each split will have hiddenSize number. First two dims are batch and seqlen, we split at dim 2 which is the embedding dim.\n",
    "        # we have a single head, so no need to do anything\n",
    "        return self.qkvProjections(batch).split(self.hiddenSize, dim=2)\n",
    "        \n",
    "    def scaledDotProduct(self, Q: BatchQueries, K: BatchKeys, V: BatchValues, mask: Optional[AttentionMask]) -> Tuple[BatchAttentions, BatchAttentionLogits]:\n",
    "\n",
    "        # some definitions to help understand the dimensions\n",
    "        # T = seqLen = one input sequence length. T is a convention\n",
    "        # seqLen = number of queries\n",
    "        # seqLen = number of keys in this implementation as every token is attending to every token in the sequence including itself\n",
    "        # seqLen = number of values in this implementation as every token is attending to every token in the sequence including itself\n",
    "        # d_q = size of a query vector\n",
    "        # d_k = size of a key vector\n",
    "        # d_v = size of a value vector\n",
    "        # in our implementation, d_q == d_k == d_v\n",
    "        # so each input batch is of size = (batch, T, d_q)\n",
    "        \n",
    "        \n",
    "        # Three steps of self.attention\n",
    "        # 1. QK_T\n",
    "        # 2. Apply attention mask on QK_T\n",
    "        # 3. softmax\n",
    "        # 4. calculate attention value QK_T V\n",
    "        \n",
    "        # qkT = torch.matmul(Q, K.transpose(-2, -1) # batch, seqlen, d_k. we transpose the last two dimensions of K\n",
    "        # batch, seqlen, d_k. we transpose the last two dimensions of K\n",
    "        # @ is a matmul operator \n",
    "        QK_T = Q @ K.transpose(-2, -1) # size = batch, T, T, we have T queries, each query as T keys.\n",
    "        QK_T /= math.sqrt(self.d_k) # scaled\n",
    "\n",
    "        if mask is not None:\n",
    "            # now there are three ways we can apply mask, and 1 way we cannot\n",
    "            # we cannot zero out the mask values as our weight loggits can be negative, so the softmax will significantly change if we zero out the masked loggits\n",
    "            # we can set masked attentions to -inf with masked_fill \n",
    "            # or we can add -inf to the masked positions\n",
    "            # with fill\n",
    "            maskCondition = mask == 0\n",
    "            QK_T = QK_T.masked_fill(maskCondition, -1.0e9) # may we get overflow? depends on our float size\n",
    "            # with add\n",
    "            # QK_T -= (1 - mask) * 1e9\n",
    "\n",
    "        attnWeights = F.softmax(QK_T, dim=-1) # softmax for each query scores. last dim has each query scores\n",
    "        # @ == matmul # batch, \n",
    "        attn = attnWeights @ V # batch, seqlen , d_v # each query has 1 attn-value vector\n",
    "        return attn, QK_T \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2586b37f-19bc-495f-94e4-8837044def10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 10])\n",
      "torch.Size([1, 2, 10]) torch.Size([1, 2, 10])\n",
      "All good\n"
     ]
    }
   ],
   "source": [
    "# Self Attention Tests\n",
    "\n",
    "def testSingleHead():\n",
    "\n",
    "    # no mask test, batch_size = 1\n",
    "    singleHead = CasualSingleHead(sentenceLength, hiddenSize=3) \n",
    "    Q = torch.tensor([[[1, 2, 3], [4, 5, 6]]], dtype=torch.float) #(batch=1, seqlen=2, d_q=3)\n",
    "    K = torch.tensor([[[1, 1, 1], [2, 2, 2]]], dtype=torch.float) #(batch=1, seqlen=2, d_k=3)\n",
    "    V = torch.tensor([[[1, 1, 1], [2, 2, 2]]], dtype=torch.float) #(batch=1, seqlen=2, d_v=3)\n",
    "    attentions, attentionLogits = singleHead.scaledDotProduct(Q, K, V, None)\n",
    "\n",
    "    d_k = 3\n",
    "    expectedLogits = torch.tensor([[[ 6., 12.], [15., 30.]]]) / math.sqrt(d_k)\n",
    "    expectedAttentions = F.softmax(expectedLogits, dim=-1) @ V\n",
    "    \n",
    "    # print(\"got\", attentionLogits)\n",
    "    # print(\"expected\", expectedLogits)\n",
    "    \n",
    "    assert np.allclose(attentionLogits, expectedLogits)\n",
    "    assert np.allclose(attentions, expectedAttentions)\n",
    "\n",
    "    # with mask test, batch_size = 1\n",
    "\n",
    "    mask = torch.tensor([[[1, 0], [1, 1]]]) #(batch=1, seqlen=2, seqlen=2) # first query attends to first key, second query attends to first and second key\n",
    "    attentions, attentionLogits = singleHead.scaledDotProduct(Q, K, V, mask)\n",
    "    \n",
    "    expectedLogits = torch.tensor([[[ 6., 12.], [15., 30.]]]) / math.sqrt(d_k)\n",
    "    expectedLogits[0][0][1] = -1.0e9 # first batch, first query, second mask\n",
    "    expectedAttentions = F.softmax(expectedLogits, dim=-1) @ V\n",
    "    \n",
    "    # print(\"got\", attentionLogits)\n",
    "    # print(\"expected\", expectedLogits)\n",
    "    assert np.allclose(attentionLogits, expectedLogits)\n",
    "    assert np.allclose(attentions, expectedAttentions)\n",
    "\n",
    "    # no mask test, batch_size = 2\n",
    "    singleHead = SingleHead(sentenceLength, hiddenSize=3) \n",
    "    Q = torch.tensor([[[1, 2, 3], [4, 5, 6]],\n",
    "                     [[10, 20, 30], [40, 50, 60]]], dtype=torch.float) #(batch=2, seqlen=2, d_q=3)\n",
    "    K = torch.tensor([[[1, 1, 1], [2, 2, 2]],\n",
    "                     [[-1, -1, -1], [-2, -2, -2]]], dtype=torch.float) #(batch=2, seqlen=2, d_k=3)\n",
    "    V = torch.tensor([[[1, 1, 1], [2, 2, 2]],\n",
    "                     [[-1, -1, -1], [-2, -2, -2]]], dtype=torch.float) #(batch=2, seqlen=2, d_v=3)\n",
    "    attentions, attentionLogits = singleHead.scaledDotProduct(Q, K, V, None)\n",
    "\n",
    "    d_k = 3\n",
    "    expectedLogits = torch.tensor([[[ 6., 12.], [15., 30.]],\n",
    "                                  [[ -60., -120.],[-150., -300.]]]) / math.sqrt(d_k)\n",
    "    expectedAttentions = F.softmax(expectedLogits, dim=-1) @ V\n",
    "\n",
    "    # print(\"got\", attentionLogits)\n",
    "    # print(\"expected\", expectedLogits)\n",
    "    \n",
    "    assert np.allclose(attentionLogits, expectedLogits)\n",
    "    assert np.allclose(attentions, expectedAttentions)\n",
    "\n",
    "\n",
    "\n",
    "def testProjections():\n",
    "\n",
    "    batchSize = 1\n",
    "    seqLen = 2\n",
    "    hiddenSize = 10\n",
    "    d_tokenEmbedding = 2\n",
    "    batch = torch.tensor([[[1, 1], [2, 2]]], dtype=torch.float) # batch, seq len, input_dim\n",
    "    \n",
    "    singleHead = CasualSingleHead(d_tokenEmbedding, hiddenSize=hiddenSize) \n",
    "    \n",
    "    layer = singleHead.qkvProjections # d_input = 2, d_qkv = 3\n",
    "    output = layer(batch)\n",
    "    expectQ, expectK, expectV = output.split(10, dim=2)\n",
    "    \n",
    "    \n",
    "    batchQueries, batchKeys, batchValues = singleHead.getBatchQKV(batch)\n",
    "    assert batchSize == batchQueries.shape[0]\n",
    "    assert seqLen == batchQueries.shape[1]\n",
    "    assert hiddenSize == batchQueries.shape[2]\n",
    "    \n",
    "    assert batchSize == batchKeys.shape[0]\n",
    "    assert seqLen == batchKeys.shape[1]\n",
    "    assert hiddenSize == batchKeys.shape[2]\n",
    "    \n",
    "    assert batchSize == batchValues.shape[0]\n",
    "    assert seqLen == batchValues.shape[1]\n",
    "    assert hiddenSize == batchValues.shape[2]\n",
    "\n",
    "    assert torch.allclose(expectQ, batchQueries)\n",
    "    assert torch.allclose(expectK, batchKeys)\n",
    "    assert torch.allclose(expectV, batchValues)\n",
    "\n",
    "def testForward():\n",
    "    batchSize = 1\n",
    "    seqLen = 2\n",
    "    hiddenSize = 10\n",
    "    d_tokenEmbedding = 2\n",
    "    batch = torch.tensor([[[1, 1], [2, 2]]], dtype=torch.float) # batch, seq len, input_dim\n",
    "    singleHead = CasualSingleHead(d_tokenEmbedding, hiddenSize=hiddenSize) \n",
    "    attention, attentionLoggits = singleHead.forward(batch, None)\n",
    "    assert attention.shape == (batchSize, seqLen, hiddenSize)\n",
    "    assert attentionLoggits.shape == (batchSize, seqLen, seqLen)\n",
    "    \n",
    "    \n",
    "def test():\n",
    "    testSingleHead()\n",
    "    testProjections()\n",
    "    testForward()\n",
    "    print(\"All good\")\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d85418d0-6ecd-4e0b-b617-494f01751dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 30]) 3 torch.Size([1, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(2, 3*10) # d_input = 2, d_qkv = 3\n",
    "batch = torch.tensor([[[1, 1], [2, 2]]], dtype=torch.float) # batch, seq len, input_dim\n",
    "output = layer(batch)\n",
    "splits = output.split(10, dim=2)\n",
    "# now each split has batch, seqlen,  10 dimensions\n",
    "# output, splits\n",
    "print(output.shape, len(splits), splits[0].shape)\n",
    "singleHead = CasualSingleHead(2, hiddenSize=10) \n",
    "singleHead(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f17870e8-e4c1-4909-aa45-b89de48f3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q = torch.tensor([[[1, 2, 3], [4, 5, 6]]], dtype=torch.float) #(seqlen, embeddingDim)\n",
    "# K = torch.tensor([[[1, 1, 1], [2, 2, 2]]], dtype=torch.float)\n",
    "# Q, K\n",
    "# Q = torch.tensor([[[1, 2, 3], [4, 5, 6]],\n",
    "#                  [[10, 20, 30], [40, 50, 60]]], dtype=torch.float) #(batch=2, seqlen=2, d_q=3)\n",
    "# K = torch.tensor([[[1, 1, 1], [2, 2, 2]],\n",
    "#                  [[-1, -1, -1], [-2, -2, -2]]], dtype=torch.float) #(batch=2, seqlen=2, d_k=3)\n",
    "# V = torch.tensor([[[1, 1, 1], [2, 2, 2]],\n",
    "#                  [[-1, -1, -1], [-2, -2, -2]]], dtype=torch.float) #(batch=2, seqlen=2, d_v=3)\n",
    "# torch.matmul(Q, K.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "354bf43e-0112-4ea7-b71f-ef28486d3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T torch.Size([1, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.4641,  6.9282],\n",
       "         [ 8.6603, 17.3205]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleHead = CasualSingleHead(sentenceLength, hiddenSize=3) \n",
    "Q = torch.tensor([[[1, 2, 3], [4, 5, 6]]], dtype=torch.float) #(batch=1, seqlen=2, d_q=3)\n",
    "K = torch.tensor([[[1, 1, 1], [2, 2, 2]]], dtype=torch.float) #(batch=1, seqlen=2, d_k=3)\n",
    "V = torch.tensor([[[1, 1, 1], [2, 2, 2]]], dtype=torch.float) #(batch=1, seqlen=2, d_v=3)\n",
    "attentions, attentionLogits = singleHead.scaledDotProduct(Q, K, V, None)\n",
    "attentionLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "62501cb4-602b-429c-ad5e-8cc00549c877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353554d8-c34a-4961-b1af-eae96e641302",
   "metadata": {},
   "outputs": [],
   "source": [
    "singleHead.forward("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
