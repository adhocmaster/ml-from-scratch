{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ceeee90-6c94-49ba-b060-85cf304f2205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dirF:\\ml-from-scratch\n"
     ]
    }
   ],
   "source": [
    "exec(open('init_notebook.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b8c3e8-458d-4172-8269-9a3255a0318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68d8a9-37ea-40f1-ad18-546dca6dd961",
   "metadata": {},
   "source": [
    "# Resources\n",
    "1. Generative Deep Learning by David Foster\n",
    "2. https://github.com/karpathy/minGPT/blob/master/demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75e886-fb06-4a1e-9964-c73d79f78de1",
   "metadata": {},
   "source": [
    "GPT is a **decoder** only model (pretraining model to be used by different decoders). We will build a casual masked language model (Casual MLM)\n",
    "\n",
    "# PipeLine\n",
    "1. Preprocessing Text -> Tokenize -> Sentence Clipper/Padder -> Input Dataset\n",
    "2. Input Dataset -> token embedding + positional embedding -> input embedding\n",
    "3. Input Sequence Embeddings -> self-attention A(Q,K,V) -> single head\n",
    "4. Single Heads -> concat attentions -> Multihead\n",
    "\n",
    "## Encoder\n",
    "5. multi-head + query -> layer-norm -> FFNs -> layer-norm -> output next token\n",
    "\n",
    "In GPT, both token and positional embeddings are learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ce282-3444-48fb-a609-95cf430cdc81",
   "metadata": {},
   "source": [
    "**This is the GPT implementation in Pytorch. The book has the tensorflow+keras implementation. Borrowed some ideas from minGPT**\n",
    "### Dependencies:\n",
    "1. Pytorch 2.x (should work in 1.x, too, as we are using as less as possible from torch)\n",
    "2. Pandas\n",
    "3. scikit-learn\n",
    "4. tqdm\n",
    "5. numpy\n",
    "6. matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "817f9a00-3a44-4e20-9573-d78665d58a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from typing import *\n",
    "import numpy.typing as npt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import heapq\n",
    "\n",
    "# Custom types\n",
    "Index = int\n",
    "Char = str\n",
    "Path = str\n",
    "Sentence = str\n",
    "TokenizedSentence = List[int]\n",
    "Word = str\n",
    "TokenId = int\n",
    "Corpus = List[Sentence]\n",
    "TokenizedCorpus = List[TokenizedSentence]\n",
    "Vocabulary = Dict[Word, TokenId]\n",
    "\n",
    "# Torch types, we use numpy typing\n",
    "SequenceLength = int\n",
    "EmbeddingDim = int\n",
    "Vector = npt.NDArray \n",
    "SequenceEmbedding = npt.NDArray[Vector] # [seqLen, embeddingDim]\n",
    "SequenceBatch = npt.NDArray[SequenceEmbedding] # [batch, seqlen, embeddingDim]\n",
    "\n",
    "# Attention types\n",
    "Query = Vector\n",
    "Queries = npt.NDArray[Query]\n",
    "BatchQueries = npt.NDArray[Queries]\n",
    "Key = Vector\n",
    "Keys = npt.NDArray[Key]\n",
    "BatchKeys = npt.NDArray[Keys]\n",
    "Value = Vector\n",
    "Values = npt.NDArray[Value]\n",
    "BatchValues = npt.NDArray[Values]\n",
    "AttentionMask = npt.NDArray[SequenceEmbedding] # [batch, seqlen, embeddingDim]\n",
    "\n",
    "Attention = Value # a single weighted value embedding for a query\n",
    "Attentions = npt.NDArray[Attention] # for all the queries of a sentence\n",
    "BatchAttentions = npt.NDArray[Attentions] # for a batch of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff028ae5-595d-4768-894d-bb6ec280ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "dataDir = \"E:/Datasets/wine-reviews\"\n",
    "vocabSize = 10_000\n",
    "sentenceLength = 100\n",
    "embeddingDim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458a95b1-645a-4959-a86f-b2c46855221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "############# The Pipeline class which is the orchestration of each step of the process ###############\n",
    "#######################################################################################################\n",
    "class Pipeline:\n",
    "    def __init__(self, vocabSize: int, sentenceLength: int, embeddingDim: int):\n",
    "        self.vocabSize = vocabSize\n",
    "        self.sentenceLength = sentenceLength\n",
    "        self.embeddingDim = embeddingDim\n",
    "        pass\n",
    "        \n",
    "    def save(self, directory: Path):\n",
    "        pass\n",
    "        \n",
    "    def preprocess(self, corpus: Corpus) -> Corpus:\n",
    "        pass\n",
    "    def tokenize(self, corpus: Corpus, vocabSize: int) -> Tuple[Vocabulary, TokenizedCorpus]:\n",
    "        pass\n",
    "    def createDatasets(self, tokenizedCorpus: TokenizedCorpus, sentenceLength) -> Tuple[Dataset, Dataset]:\n",
    "        pass\n",
    "    \n",
    "pipeline = Pipeline(vocabSize, sentenceLength, embeddingDim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe1634-a975-44b1-9d7c-9bf8cc1d8e44",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing\n",
    "1. read text,\n",
    "2. pad punctuations with spaces to convert them to individual words.                                                                                                                                                                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d82e07b-8ca5-401d-aabc-58dc4ac99729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dataDir, \"winemag-data-130k-v2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32dd067d-1759-4d0d-9851-d7cd2b8eb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e8f1a4-278a-459c-b87e-5b100323a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"description\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bab086c-047e-4b33-b51d-a0ca5c39a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aa0f1ec-afb1-483b-8450-f15ddadeb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessorText:\n",
    "    \n",
    "    def padPunk(self, s: Sentence) -> Sentence: # this does not work well. We will do BPE next. isn't -> becomes bad is ' t. now t is a word! It's it ' s\n",
    "        s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s) #to words\n",
    "        s = re.sub(' +', ' ', s) # multiple spaces to one\n",
    "        return s\n",
    "    \n",
    "    def smallCase(self, corpus: List[Sentence]) -> List[Sentence]:\n",
    "        return [s.lower() for s in corpus]\n",
    "\n",
    "    def __call__(self, corpus: List[Sentence]) -> List[Sentence]:\n",
    "        corpus = [self.padPunk(s) for s in corpus] \n",
    "        return self.smallCase(corpus)\n",
    "        \n",
    "pipeline.preprocess = lambda corpus: PreProcessorText()(corpus)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e84cabff-ecc4-4dcd-9f8e-06cb3585509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pipeline.preprocess(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f2f1306-bd8b-46c2-9ca8-4f4ca0ab821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"aromas include tropical fruit , broom , brimstone and dried herb . the palate isn ' t overly expressive , offering unripened apple , citrus and dried sage alongside brisk acidity . \",\n",
       " \"this is ripe and fruity , a wine that is smooth while still structured . firm tannins are filled out with juicy red berry fruits and freshened with acidity . it ' s already drinkable , although it will certainly be better from 2016 . \",\n",
       " 'tart and snappy , the flavors of lime flesh and rind dominate . some green pineapple pokes through , with crisp acidity underscoring the flavors . the wine was all stainless - steel fermented . ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb9841-70b8-41e8-a09d-423710ceaf6e",
   "metadata": {},
   "source": [
    "## 1.2 Tokenize\n",
    "1. vocabulary\n",
    "2. tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe3db34a-0177-4972-9995-60b65ab3fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTokenizer:\n",
    "    \n",
    "    def getVocabulary(self, corpus: Corpus, vocabSize: int) -> Vocabulary:\n",
    "        vectorizer = CountVectorizer().fit(corpus)\n",
    "        topWords = heapq.nlargest(vocabSize-1, vectorizer.vocabulary_, key=lambda w: vectorizer.vocabulary_[w])\n",
    "        wordToToken = {}\n",
    "        for idx, w in enumerate(topWords):\n",
    "            wordToToken[w] = idx\n",
    "    \n",
    "        wordToToken['UNK'] = len(wordToToken)\n",
    "        print(f\"Created a vocabulary with top {len(wordToToken)} words from {len(vectorizer.vocabulary_)} words with UNK as the last word\")\n",
    "        return wordToToken\n",
    "\n",
    "    def tokenizeSentence(self, sentence: Sentence, vocabulary: Vocabulary) -> TokenizedSentence:\n",
    "        words = sentence.split()\n",
    "        tSen = [\n",
    "            vocabulary[w] if w in vocabulary\n",
    "            else vocabulary[\"UNK\"]\n",
    "                for w in words\n",
    "        ]\n",
    "        return tSen\n",
    "\n",
    "    def build(self, corpus: Corpus, vocabSize: int) -> Tuple[Vocabulary, TokenizedCorpus]:\n",
    "        vocabulary = self.getVocabulary(corpus, vocabSize)\n",
    "        tokenizedCorpus = [self.tokenizeSentence(s, vocabulary) for s in corpus]\n",
    "        return vocabulary, tokenizedCorpus\n",
    "\n",
    "    def __call__(self, corpus: Corpus, vocabSize: int) -> Tuple[Vocabulary, TokenizedCorpus]:\n",
    "        return self.build(corpus, vocabSize)\n",
    "\n",
    "pipeline.tokenize = lambda corpus: SentenceTokenizer()(corpus, pipeline.vocabSize)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd5e84f-96dd-4cb7-b5be-71ab84b115b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a vocabulary with top 10000 words from 31274 words with UNK as the last word\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3415"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, tokenizedCorpus = pipeline.tokenize(corpus)\n",
    "# sentenceTokenizer = SentenceTokenizer()\n",
    "# vocabulary, tokenizedCorpus = sentenceTokenizer.build(corpus, vocabSize)\n",
    "vocabulary[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3970c0d3-7d13-45f8-b640-43c4eefb71cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9999, 9999, 2608, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 3415, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 1901, 9999, 9999, 9999, 9999, 9999, 7486, 9999, 9999, 9999, 9999]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizedCorpus[0]) # mostl UNKs as we have over 30 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552189f9-22cf-4ce8-942b-8387490dbda6",
   "metadata": {},
   "source": [
    "## 1.3 Dataset\n",
    "1. fixed length sentences\n",
    "2. torch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bc8efcc-1578-4b16-b8c3-4ac226007fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, tokenizedCorpus: TokenizedCorpus, split: str, length: int, vocabSize: int):\n",
    "        assert split in {'train', 'test'} # borrowed from minGPT\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "        self.vocabSize = vocabSize\n",
    "        self.corpus = tokenizedCorpus.copy()\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.size = int(len(self.corpus) * 0.7)\n",
    "        else:\n",
    "            self.size = len(self.corpus) - int(len(self.corpus) * 0.7)\n",
    "            \n",
    "        self.ids = self._generateIds()\n",
    "        self._reshapeCorpus()\n",
    "        \n",
    "\n",
    "    def _reshapeCorpus(self):\n",
    "        # converts to fixed length sentences by clipping or padding\n",
    "        for idx in tqdm(self.ids, desc=f\"reshaping {self.split} corpus\"):\n",
    "            self.corpus[idx] = self._reshapeSentence(self.corpus[idx])\n",
    "        pass\n",
    "\n",
    "    def _reshapeSentence(self, sentence: TokenizedSentence) -> TokenizedSentence:\n",
    "        if len(sentence) >= self.length:\n",
    "            return sentence[:self.length]\n",
    "        # we pad with zeros. # zero not in vocab\n",
    "        return sentence + [0] * (self.length - len(sentence))\n",
    "        \n",
    "\n",
    "    def _generateIds(self) -> List[Index]:\n",
    "        # we just get the top for train and bot for test.\n",
    "        if self.split == 'train':\n",
    "            return list(range(self.size))\n",
    "        else:\n",
    "            start = int(len(self.corpus) * 0.7) # train index ends before start\n",
    "            return [i+start for i in range(self.size)]\n",
    "\n",
    "    def getVocabSize(self) -> int:\n",
    "        return self.vocabSize\n",
    "\n",
    "    def getBlockSize(self) -> int:\n",
    "        # as this is an encoder. we feed n inputs, n-1 outputs, and then read outputs? need more clarifications.\n",
    "        return self.vocabSize * 2 - 1\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx: Index) -> TokenizedSentence:\n",
    "        return self.corpus[self.ids[idx]] # ids can be sparse\n",
    "\n",
    "pipeline.createDatasets = lambda tokenizedCorpus: (SentenceDataset(tokenizedCorpus, \"train\", pipeline.sentenceLength, pipeline.vocabSize), SentenceDataset(tokenizedCorpus, \"test\", pipeline.sentenceLength, pipeline.vocabSize))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0f5870a-bd08-427a-9308-c34456521c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reshaping train corpus: 100%|████████████████████████████████████████████████| 90979/90979 [00:00<00:00, 269965.73it/s]\n",
      "reshaping test corpus: 100%|█████████████████████████████████████████████████| 38992/38992 [00:00<00:00, 433116.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# trainSet = SentenceDataset(tokenizedCorpus, \"train\", 100)\n",
    "# testSet = SentenceDataset(tokenizedCorpus, \"test\", 100)\n",
    "trainSet, testSet = pipeline.createDatasets(tokenizedCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06863c93-77f9-4e55-860e-15a56c5b2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert trainSet.ids[-1] + 1 == testSet.ids[0]\n",
    "assert testSet.ids[-1] + 1 == len(tokenizedCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1cbf96-9386-4708-a7c3-499c9d4bd322",
   "metadata": {},
   "source": [
    "## 2. Embedding\n",
    "We will learn both token embedding and positional encoding. However, we can also use pretrained embedding and trigonometric functions (sine, cosine) from the original Transformer for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a383972-e289-4c74-bcfb-70f76a5a5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenEmbedding = nn.Embedding(vocabSize, embeddingDim)\n",
    "# posEmbedding = nn.Embedding(sentenceLength, embeddingDim) # number of positions. the dim is the same as tokens as they will be summed\n",
    "pipeline.createEmbedding = lambda : (nn.Embedding(pipeline.vocabSize, pipeline.embeddingDim), nn.Embedding(pipeline.sentenceLength, pipeline.embeddingDim))\n",
    "tokenEmbedding, posEmbedding = pipeline.createEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5371eb35-e34f-4f1e-93e6-5fec2f739407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(10000, 50), Embedding(100, 50))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenEmbedding, posEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ff114-5449-4fa0-b900-9da3d0863a4c",
   "metadata": {},
   "source": [
    "## 3. The Single Head\n",
    "**From this point on, everything will have a batch dimension at 0**\n",
    "1. Input Embedding\n",
    "2. Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4501d0-c960-4b83-b43c-71e099bb5b8c",
   "metadata": {},
   "source": [
    "### 3.1 Input Embedding\n",
    "1. Convert the sentence token batch to embedding batch\n",
    "2. Create the casual masked batch for every sentence embedding so that the future tokens are not visible. Mask is added after attention scores are computed to save computation\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1786ff6-2835-43b4-80a5-f50c2b20bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputProcessor:\n",
    "    def embed(self, batch: List[TokenizedSentence]) -> SequenceBatch:\n",
    "        pass\n",
    "\n",
    "class SingleHead(nn.Module):\n",
    "    def __init__(self, inputSize: int, hiddenSize: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.dk = self.hiddenSize\n",
    "        # stacked ffns for Q, K, V projections\n",
    "        self.qkvProjections = nn.Linear(inputSize, 3 * hiddenSize) # [0- hiddenSize] for q, [hiddenSize - 2* hiddenSize] for k, [2* hiddenSize - 3 * hiddenSize] for v.\n",
    "        self.register_buffer(mask\n",
    "        \n",
    "        \n",
    "    def scaledDotProduct(self, Q: BatchQueries, K: BatchKeys, V: BatchValues, mask: Optional[AttentionMask]) -> BatchAttentions:\n",
    "        # Three steps of self.attention\n",
    "        # 1. QK_T\n",
    "        # 2. Apply attention mask on QK_T\n",
    "        # 3. softmax\n",
    "        # 4. calculate attention value QK_T V\n",
    "        \n",
    "        # qkT = torch.matmul(Q, K.transpose(-2, -1) # batch, seqlen, d_k. we transpose the last two dimensions of K\n",
    "        # batch, seqlen, d_k. we transpose the last two dimensions of K\n",
    "        QK_T = Q @ K.transpose(-2, -1) # @ is a matmul operator \n",
    "        QK_T /= self.dk\n",
    "        print(\"QK_T\", QK_T.shape)\n",
    "\n",
    "        if mask is not None:\n",
    "            # now there are three ways we can apply mask, and 1 way we cannot\n",
    "            # we cannot zero out the mask values as our weight loggits can be negative, so the softmax will significantly change if we zero out the masked loggits\n",
    "            # we can set masked attentions to -inf with masked_fill \n",
    "            # or we can add -inf to the masked positions\n",
    "            # with fill\n",
    "            maskCondition = mask == 0\n",
    "            QK_T = QK_T.masked_fill(maskCondition, -1e9) # may we get overflow? depends on our float size\n",
    "            # with add\n",
    "            # QK_T -= (1 - mask) * 1e9\n",
    "\n",
    "        attn = F.softmax(QK_T, dim=-1) # last dim has each query scores\n",
    "        return attn @ V # batch, seqlen, \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2586b37f-19bc-495f-94e4-8837044def10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34f67b56-3f22-4162-b172-447b2b3b3ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(3, 3)).view((3,3,1))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "04f50565-7cac-4cb7-a7b7-9c9ba3ed7829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 2., 3.],\n",
       "          [4., 5., 6.]]]),\n",
       " tensor([[[1., 1., 1.],\n",
       "          [2., 2., 2.]]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.tensor([[[1, 2, 3], [4, 5, 6]]], dtype=torch.float) #(seqlen, embeddingDim)\n",
    "K = torch.tensor([[[1, 1, 1], [2, 2, 2]]], dtype=torch.float)\n",
    "Q, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8584a24a-45eb-40e9-801e-5140bd98e4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6., 12.],\n",
       "         [15., 30.]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(Q, K.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b90c501-54a9-4999-b565-122bfefde0f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,2,3) and (3,2,1) not aligned: 3 (dim 2) != 2 (dim 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]]])\n\u001b[0;32m      2\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]]])\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,2,3) and (3,2,1) not aligned: 3 (dim 2) != 2 (dim 1)"
     ]
    }
   ],
   "source": [
    "a = np.asarray([[[1, 2, 3], [4, 5, 6]]])\n",
    "b = np.asarray([[[1, 1, 1], [2, 2, 2]]])\n",
    "np.dot(a, b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "354bf43e-0112-4ea7-b71f-ef28486d3783",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scaledDotProduct() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m K \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m      4\u001b[0m V \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m----> 5\u001b[0m attentions \u001b[38;5;241m=\u001b[39m \u001b[43msingleHead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaledDotProduct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: scaledDotProduct() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "singleHead = SingleHead(sentenceLength, hiddenSize=3) \n",
    "Q = torch.tensor([[[1, 2, 3], [4, 5, 6]]], dtype=torch.float) #(seqlen, embeddingDim)\n",
    "K = torch.tensor([[[1, 1, 1], [2, 2, 2]]], dtype=torch.float)\n",
    "V = torch.tensor([[[1, 1, 1], [2, 2, 2]]], dtype=torch.float)\n",
    "attentions = singleHead.scaledDotProduct(Q, K, V, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0343f20-a202-4f3d-a26e-ac3b96596592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(3, 3)).view((1,3,3))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "820ac3ee-1edd-43dc-936b-ebf7e8696986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000, 0.5000, 0.2500],\n",
       "          [0.5000, 1.0000, 0.7000],\n",
       "          [0.1000, 0.2000, 1.0000]]]),\n",
       " torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = torch.tensor([[1, 0.5, 0.25], [0.5, 1, 0.7], [0.1, 0.2, 1]], dtype=torch.float)\n",
    "attn = attn.unsqueeze(dim=0) # adding the batch dim\n",
    "attn, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62501cb4-602b-429c-ad5e-8cc00549c877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
