{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ceeee90-6c94-49ba-b060-85cf304f2205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dirF:\\ml-from-scratch\n"
     ]
    }
   ],
   "source": [
    "exec(open('init_notebook.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b8c3e8-458d-4172-8269-9a3255a0318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68d8a9-37ea-40f1-ad18-546dca6dd961",
   "metadata": {},
   "source": [
    "# Resources\n",
    "1. Generative Deep Learning by David Foster\n",
    "2. https://github.com/karpathy/minGPT/blob/master/demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4054816e-d1d5-4178-8e3b-2cfd0f2abe10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**This is the GPT implementation in Pytorch. The book has the tensorflow+keras implementation. Borrowed some ideas from minGPT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75e886-fb06-4a1e-9964-c73d79f78de1",
   "metadata": {},
   "source": [
    "GPT is a encoder only model (pretraining model to be used by different decoders)\n",
    "\n",
    "# PipeLine\n",
    "1. Preprocessing Text -> Tokenize -> Sentence Clipper/Padder -> Input Dataset\n",
    "2. Input Dataset -> token embedding + positional embedding -> input embedding\n",
    "3. Input Sequence Embeddings -> self-attention A(Q,K,V) -> single head\n",
    "4. Single Heads -> concat attentions -> Multihead\n",
    "\n",
    "## Encoder\n",
    "5. multi-head + query -> layer-norm -> FFNs -> layer-norm -> output next token\n",
    "\n",
    "In GPT, both token and positional embeddings are learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "817f9a00-3a44-4e20-9573-d78665d58a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from typing import *\n",
    "import numpy.typing as npt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import heapq\n",
    "\n",
    "# Custom types\n",
    "Index = int\n",
    "Char = str\n",
    "Path = str\n",
    "Sentence = str\n",
    "TokenizedSentence = List[int]\n",
    "Word = str\n",
    "TokenId = int\n",
    "Corpus = List[Sentence]\n",
    "TokenizedCorpus = List[TokenizedSentence]\n",
    "Vocabulary = Dict[Word, TokenId]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ff028ae5-595d-4768-894d-bb6ec280ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "dataDir = \"E:/Datasets/wine-reviews\"\n",
    "vocabSize = 10_000\n",
    "sentenceLength = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "458a95b1-645a-4959-a86f-b2c46855221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "############# The Pipeline class which is the orchestration of each step of the process ###############\n",
    "#######################################################################################################\n",
    "class Pipeline:\n",
    "    def save(self, directory: Path):\n",
    "        pass\n",
    "    def preprocess(self, corpus: Corpus) -> Corpus:\n",
    "        pass\n",
    "    def tokenize(self, corpus: Corpus, vocabSize: int) -> Tuple[Vocabulary, TokenizedCorpus]:\n",
    "        pass\n",
    "    def createDatasets(self, tokenizedCorpus: TokenizedCorpus, sentenceLength) -> Tuple[SentenceDataset, SentenceDataset]:\n",
    "        pass\n",
    "    \n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe1634-a975-44b1-9d7c-9bf8cc1d8e44",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing\n",
    "1. read text,\n",
    "2. pad punctuations with spaces to convert them to individual words.                                                                                                                                                                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d82e07b-8ca5-401d-aabc-58dc4ac99729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dataDir, \"winemag-data-130k-v2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32dd067d-1759-4d0d-9851-d7cd2b8eb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "84e8f1a4-278a-459c-b87e-5b100323a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"description\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2bab086c-047e-4b33-b51d-a0ca5c39a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1aa0f1ec-afb1-483b-8450-f15ddadeb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessorText:\n",
    "    \n",
    "    def padPunk(self, s: Sentence) -> Sentence: # this does not work well. We will do BPE next. isn't -> becomes bad is ' t. now t is a word! It's it ' s\n",
    "        s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s) #to words\n",
    "        s = re.sub(' +', ' ', s) # multiple spaces to one\n",
    "        return s\n",
    "    \n",
    "    def smallCase(self, corpus: List[Sentence]) -> List[Sentence]:\n",
    "        return [s.lower() for s in corpus]\n",
    "\n",
    "    def __call__(self, corpus: List[Sentence]) -> List[Sentence]:\n",
    "        corpus = [self.padPunk(s) for s in corpus] \n",
    "        return self.smallCase(corpus)\n",
    "        \n",
    "pipeline.preprocess = lambda corpus: PreProcessorText()(corpus)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e84cabff-ecc4-4dcd-9f8e-06cb3585509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pipeline.preprocess(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2f2f1306-bd8b-46c2-9ca8-4f4ca0ab821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"aromas include tropical fruit , broom , brimstone and dried herb . the palate isn ' t overly expressive , offering unripened apple , citrus and dried sage alongside brisk acidity . \",\n",
       " \"this is ripe and fruity , a wine that is smooth while still structured . firm tannins are filled out with juicy red berry fruits and freshened with acidity . it ' s already drinkable , although it will certainly be better from 2016 . \",\n",
       " 'tart and snappy , the flavors of lime flesh and rind dominate . some green pineapple pokes through , with crisp acidity underscoring the flavors . the wine was all stainless - steel fermented . ']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb9841-70b8-41e8-a09d-423710ceaf6e",
   "metadata": {},
   "source": [
    "## 1.2 Tokenize\n",
    "1. vocabulary\n",
    "2. tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fe3db34a-0177-4972-9995-60b65ab3fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTokenizer:\n",
    "    \n",
    "    def getVocabulary(self, corpus: Corpus, vocabSize: int) -> Vocabulary:\n",
    "        vectorizer = CountVectorizer().fit(corpus)\n",
    "        topWords = heapq.nlargest(vocabSize-1, vectorizer.vocabulary_, key=lambda w: vectorizer.vocabulary_[w])\n",
    "        wordToToken = {}\n",
    "        for idx, w in enumerate(topWords):\n",
    "            wordToToken[w] = idx\n",
    "    \n",
    "        wordToToken['UNK'] = len(wordToToken)\n",
    "        print(f\"Created a vocabulary with top {len(wordToToken)} words from {len(vectorizer.vocabulary_)} words with UNK as the last word\")\n",
    "        return wordToToken\n",
    "\n",
    "    def tokenizeSentence(self, sentence: Sentence, vocabulary: Vocabulary) -> TokenizedSentence:\n",
    "        words = sentence.split()\n",
    "        tSen = [\n",
    "            vocabulary[w] if w in vocabulary\n",
    "            else vocabulary[\"UNK\"]\n",
    "                for w in words\n",
    "        ]\n",
    "        return tSen\n",
    "\n",
    "    def build(self, corpus: Corpus, vocabSize: int) -> Tuple[Vocabulary, TokenizedCorpus]:\n",
    "        vocabulary = self.getVocabulary(corpus, vocabSize)\n",
    "        tokenizedCorpus = [self.tokenizeSentence(s, vocabulary) for s in corpus]\n",
    "        return vocabulary, tokenizedCorpus\n",
    "\n",
    "    def __call__(self, corpus: Corpus, vocabSize: int) -> Tuple[Vocabulary, TokenizedCorpus]:\n",
    "        return self.build(corpus, vocabSize)\n",
    "\n",
    "pipeline.tokenize = lambda corpus, vocabSize: SentenceTokenizer()(corpus, vocabSize)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dbd5e84f-96dd-4cb7-b5be-71ab84b115b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a vocabulary with top 10000 words from 31274 words with UNK as the last word\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3415"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, tokenizedCorpus = pipeline.tokenize(corpus, vocabSize)\n",
    "# sentenceTokenizer = SentenceTokenizer()\n",
    "# vocabulary, tokenizedCorpus = sentenceTokenizer.build(corpus, vocabSize)\n",
    "vocabulary[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3970c0d3-7d13-45f8-b640-43c4eefb71cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9999, 9999, 2608, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 3415, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 1901, 9999, 9999, 9999, 9999, 9999, 7486, 9999, 9999, 9999, 9999]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizedCorpus[0]) # mostl UNKs as we have over 30 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552189f9-22cf-4ce8-942b-8387490dbda6",
   "metadata": {},
   "source": [
    "## 1.3 Dataset\n",
    "1. fixed length sentences\n",
    "2. torch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5bc8efcc-1578-4b16-b8c3-4ac226007fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, tokenizedCorpus: TokenizedCorpus, split: str, length: int):\n",
    "        assert split in {'train', 'test'} # borrowed from minGPT\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "        self.corpus = tokenizedCorpus.copy()\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.size = int(len(self.corpus) * 0.7)\n",
    "        else:\n",
    "            self.size = len(self.corpus) - int(len(self.corpus) * 0.7)\n",
    "            \n",
    "        self.ids = self._generateIds()\n",
    "        self._reshapeCorpus()\n",
    "        \n",
    "\n",
    "    def _reshapeCorpus(self):\n",
    "        # converts to fixed length sentences by clipping or padding\n",
    "        for idx in tqdm(self.ids, desc=f\"reshaping {self.split} corpus\"):\n",
    "            self.corpus[idx] = self._reshapeSentence(self.corpus[idx])\n",
    "        pass\n",
    "\n",
    "    def _reshapeSentence(self, sentence: TokenizedSentence) -> TokenizedSentence:\n",
    "        if len(sentence) >= self.length:\n",
    "            return sentence[:self.length]\n",
    "        # we pad with zeros. # zero not in vocab\n",
    "        return sentence + [0] * (self.length - len(sentence))\n",
    "        \n",
    "\n",
    "    def _generateIds(self) -> List[Index]:\n",
    "        # we just get the top for train and bot for test.\n",
    "        if self.split == 'train':\n",
    "            return list(range(self.size))\n",
    "        else:\n",
    "            start = int(len(self.corpus) * 0.7) # train index ends before start\n",
    "            return [i+start for i in range(self.size)]\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx: Index) -> TokenizedSentence:\n",
    "        return self.corpus[self.ids[idx]] # ids can be sparse\n",
    "\n",
    "pipeline.createDatasets = lambda tokenizedCorpus, sentenceLength: (SentenceDataset(tokenizedCorpus, \"train\", sentenceLength), SentenceDataset(tokenizedCorpus, \"test\", sentenceLength))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b0f5870a-bd08-427a-9308-c34456521c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reshaping train corpus: 100%|████████████████████████████████████████████████| 90979/90979 [00:00<00:00, 471388.35it/s]\n",
      "reshaping test corpus: 100%|██████████████████████████████████████████████████| 38992/38992 [00:00<00:00, 93947.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# trainSet = SentenceDataset(tokenizedCorpus, \"train\", 100)\n",
    "# testSet = SentenceDataset(tokenizedCorpus, \"test\", 100)\n",
    "trainSet, testSet = pipeline.createDatasets(tokenizedCorpus, sentenceLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "06863c93-77f9-4e55-860e-15a56c5b2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert trainSet.ids[-1] + 1 == testSet.ids[0]\n",
    "assert testSet.ids[-1] + 1 == len(tokenizedCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24a6c0-56b2-4cd9-a693-225e8d3b715c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
