{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7313f9-d09f-4a02-8851-2b935025725b",
   "metadata": {},
   "source": [
    "# Sources\n",
    "1. https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e319cc0-9b93-4eb2-b4e1-13a5c960a48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Life is short, eat dessert first'\n",
    "\n",
    "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad133a82-83d6-45f1-a056-b5ef58b3e9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 5, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2d3038-4592-4d5d-a02e-4f545d13f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(6, 16)\n",
    "embeded_sentence = embed(sentence_int).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd957d4-8484-409d-937b-a50a0bb01b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec979eb9-790c-4a0f-8dc2-4fb478da04dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
       "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
       "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
       "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
       "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
       "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
       "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
       "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
       "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
       "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
       "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
       "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2522db01-9810-4d97-860f-cb658560ad31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 16])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = embeded_sentence.shape[1]\n",
    "embeded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf6e645-c102-4d7a-ad10-f06015325579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2fbf449-2115-44da-b63b-f767f19fc3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_q, d_k, d_v = 24, 24, 28 # output channel of Q, K, and V nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7746e99b-4b8d-4d63-8ed3-9f4d356c1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_query = torch.nn.Parameter(torch.rand(d_q, d))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_k, d))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_v, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aaf257-c2d9-4ca2-8839-e74718a6cdc5",
   "metadata": {},
   "source": [
    "# Query = \"is\" - the second word in our sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3225002c-8c41-4797-bd26-f56f66a938d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "x_2 = embeded_sentence[1]\n",
    "query_2 = W_query.matmul(x_2)\n",
    "key_2 = W_key.matmul(x_2)\n",
    "value_2 = W_value.matmul(x_2)\n",
    "\n",
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ee132-4f7c-46e4-9b52-73f3a8805895",
   "metadata": {},
   "source": [
    "# query(token) = W_q x embedding(token) = An FFN with arbitary output channels.\n",
    "1. If we think of the token embedding vector as the input, multiplying with W_q is creating **W_q.shape[0]** output channels, just like a FFN.  **W_q.shape[1]** is input channels and  **W_q.shape[0]** is the output channels.\n",
    "2. We create 3 FFNs, Q, K, V\n",
    "3. Q, and K needs to have the same number of output channels. Explanation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e1ebaf-a43b-4a74-b426-180957439e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 24])\n",
      "values.shape: torch.Size([6, 28])\n"
     ]
    }
   ],
   "source": [
    "keys = W_key.matmul(embeded_sentence.T).T # converting 6 tokens to their keys. the keys will be in columns, to we transpose them. now we have 6x24 \n",
    "values = W_value.matmul(embeded_sentence.T).T\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d125f9-9ae8-45b3-a838-c4a17fe4914b",
   "metadata": {},
   "source": [
    "# Attention Weight!\n",
    "## The unnormalized weight, w_i_j = (query(current_token)(i)).T           x              key(other_token)(j)\n",
    "\n",
    "Interestingly, the dot product of current token query and some other token key is a cosine similarity! So, it's going to attend to tokens which have similar keys as the current token's query! Interestingly, **there is probably only one way to express relationship between elements in NN: cosine similarity**. So, to model relationships between users and products, we need to create two new embeddings of the same length, so that we can measure their similarity. The more products we have, the more channels we need. These embeddings are jointly learned because they need to learn the relationship along with entity identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07307be4-ecd8-426f-b854-25992fc831f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3206, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_24 = query_2.dot(keys[4]) # second token to 5th token\n",
    "omega_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8389aca2-0c69-4a4f-a116-ebb014a1f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_2 = query_2.matmul(keys.T) # we saved the keys row-wise for tokens. each row -> a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d3c0885-97be-4268-84ba-df9d9110f907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -7.0847,  -4.5398,   3.9887,  10.2379,   2.3206, -10.5434],\n",
       "       grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_2 # we see that query_2 and keys[1] current does not have the highest similarity, because our Q and K networks are not trained yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ed013-953f-4275-9441-b7ef4c115d44",
   "metadata": {},
   "source": [
    "## Normalizing attention weights\n",
    "We can normalized the attention weights by softmax directly. However, the authors in \"Attention is all you need\" scaled it by (1/root(dk)). **It ensures that the euclidean length of the weight vectors to be approximately in the same magnitude. how?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1422067-683c-4bbb-a0a0-a87bcc9332dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0185, 0.0312, 0.1778, 0.6368, 0.1265, 0.0092],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# attention_weights_2_no_scaling = F.softmax(omega_2 / d_k, dim=0)\n",
    "attention_weights_2 = F.softmax(omega_2 / d_k ** 0.5, dim=0) # attention vector for query 2\n",
    "# attention_weights_2_arbitary = F.softmax(omega_2 / 100, dim=0) # arbitary is not good!\n",
    "# print(attention_weights_2_no_scaling, attention_weights_2, attention_weights_2_arbitary)\n",
    "attention_weights_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942f05c-5841-4a29-9fb3-a32dff5dc360",
   "metadata": {},
   "source": [
    "## Attention weight is a vector of size of vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c32f5b-d390-4a94-a354-02470eb59b98",
   "metadata": {},
   "source": [
    "# Now the context vector\n",
    "The context vector is the most confusing. So, each token value is multiplied by its correspoinding weight. Then the columns are summed. Don't know what it accumulates. **But more related tokens will have more information added to the context.** Context vector feels like a continuous variable, but can blend concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82510164-2cd6-4a79-b83d-5ee15cc2d48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9495, -1.4345, -2.0504, -0.3737, -1.5098, -0.5921, -0.4289, -1.9790,\n",
       "        -1.7937, -0.7146, -0.9926, -2.0061, -2.1961, -1.7174, -1.0732, -0.7900,\n",
       "        -1.7367, -2.2095, -0.9344, -1.5299, -0.2828, -0.5350, -1.7285, -1.5485,\n",
       "        -0.2043, -0.7109, -1.5165, -1.5167], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2.matmul(values)\n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f89bb5e-dc30-4e87-9b29-748621ed6c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8441,  0.2829,  1.7343,  1.6112,  2.1563,  1.1398,  1.6928,  1.5736,\n",
       "          1.7709,  0.9618,  1.3077,  0.2716,  0.3070,  0.3427,  2.4012,  1.9869,\n",
       "         -1.1107,  0.6782, -0.2181,  0.8178,  0.5018,  0.9887,  1.3350,  0.1589,\n",
       "         -0.3449,  0.9065,  1.6519, -0.3440],\n",
       "        [ 0.5165,  0.2638,  0.1946,  0.1296, -0.2176, -1.2548, -0.9272, -1.3402,\n",
       "         -0.4107, -0.0859,  1.0926,  0.4078, -0.6770,  0.1110, -1.1055,  0.3156,\n",
       "         -0.3169,  0.7937, -1.1166,  3.0497, -0.2863,  1.5513,  2.7004,  0.5483,\n",
       "         -2.4544, -1.5389, -0.4168,  0.2455],\n",
       "        [-1.4350, -3.0582, -1.3735, -1.0167, -0.9396, -2.5408, -2.1351, -1.8701,\n",
       "         -1.9994, -3.7609, -3.8755, -3.1365, -2.1639, -3.0949, -3.7118, -1.8682,\n",
       "         -1.8869, -1.7023, -1.4043, -4.1602, -3.5326, -1.8202, -3.1335, -1.7162,\n",
       "         -2.0997, -1.8854, -2.1002, -3.4872],\n",
       "        [-1.0645, -0.9245, -3.0223, -0.6932, -2.1795,  0.1399,  0.0170, -2.8164,\n",
       "         -2.1397,  0.7122, -0.7365, -1.5728, -2.6054, -1.3145, -0.8618, -0.5164,\n",
       "         -1.8432, -3.0082, -0.7213, -1.2709,  0.4588, -1.0818, -1.7762, -2.2016,\n",
       "          0.6103, -0.0420, -1.7609, -1.3318],\n",
       "        [-0.2955, -2.7453,  0.7757,  1.4735,  0.0459, -1.9004, -0.6969,  1.2049,\n",
       "         -0.9530, -4.2561,  0.8399, -3.7423, -1.1625, -2.9441,  0.7369, -1.5681,\n",
       "         -1.7600, -0.3928, -1.6623, -1.0396,  0.1901,  2.8688, -1.3868,  0.8984,\n",
       "         -1.3895, -2.6022, -0.4103, -0.6668],\n",
       "        [ 2.2329,  3.4829, -1.9674,  3.0705,  0.6728,  3.1772,  2.7996,  0.7759,\n",
       "          2.7167,  2.6194,  0.1099,  0.9618,  1.1149,  3.5639,  3.5327,  2.4810,\n",
       "          2.8085,  2.3073,  2.6020,  4.4131,  3.1466,  5.2343,  2.8615,  2.7244,\n",
       "          4.2720,  1.2620,  1.3700,  3.7709]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d8b79-5cf3-4f4f-83f2-4a3ae10577bb",
   "metadata": {},
   "source": [
    "# Multihead attention\n",
    "So far, we made one channel to produce a single context vector. Multi-head copies the single head to create multiple context vector. Input is copied multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c03e549-3e81-4bad-8608-fcc0ce3f495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = 3\n",
    "multihead_W_Q = torch.nn.Parameter(torch.rand(heads, d_q, d))\n",
    "multihead_W_K = torch.nn.Parameter(torch.rand(heads, d_k, d)) # d_k == d_q\n",
    "multihead_W_V = torch.nn.Parameter(torch.rand(heads, d_v, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54d84d7a-5d29-4e9e-8e7f-54ebe35793d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_query_2 = multihead_W_Q.matmul(x_2) # first row is regarded as the batch dim, x_2 is broadcasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ade3e90f-bcf9-47fc-801d-172fa4e31e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5317, -1.6069,  1.1446, -1.7651, -0.0069, -1.2229,  0.6265, -1.1083,\n",
       "         -0.5696, -0.2894,  0.9853, -1.1417, -0.7089, -1.5938,  0.9208,  0.1191,\n",
       "          0.5370,  0.1285, -1.1273,  0.3900,  1.6581,  0.2398, -0.4409,  0.8581],\n",
       "        [-1.7130,  0.7300,  0.3016,  1.2672, -0.4968,  1.7345, -0.4737, -0.1441,\n",
       "         -1.5281,  0.1814,  2.1658,  0.0782,  1.1053, -0.2787, -0.8150,  1.2533,\n",
       "         -1.6784, -0.6629, -0.7785, -0.4442, -0.7335, -1.0979, -1.6565, -0.4303],\n",
       "        [-0.2142,  0.3233, -0.9046, -1.1118,  1.2510,  1.1929, -0.8451,  1.8892,\n",
       "         -0.6453,  0.0580, -0.4397, -0.2499,  1.5047,  0.9832, -1.5491, -0.4635,\n",
       "          0.5074,  1.5554, -2.2974, -0.0805, -0.3201,  1.0347,  1.1994,  0.1928]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e63d4e8-d109-4461-8010-d753c97dc72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8873, 0.2034, 0.9871, 0.1758, 0.6914],\n",
      "         [0.8859, 0.6605, 0.8328, 0.6707, 0.6894],\n",
      "         [0.9387, 0.4778, 0.4763, 0.7615, 0.2538]],\n",
      "\n",
      "        [[0.9377, 0.7955, 0.9131, 0.1981, 0.6997],\n",
      "         [0.8676, 0.3539, 0.2717, 0.6077, 0.2121],\n",
      "         [0.2421, 0.4025, 0.9509, 0.3354, 0.6794]]])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.rand(2, 3, 5)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c7d44d3-00a9-416d-b635-2ed62c5a239d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8873, 0.9377],\n",
      "         [0.8859, 0.8676],\n",
      "         [0.9387, 0.2421]],\n",
      "\n",
      "        [[0.2034, 0.7955],\n",
      "         [0.6605, 0.3539],\n",
      "         [0.4778, 0.4025]],\n",
      "\n",
      "        [[0.9871, 0.9131],\n",
      "         [0.8328, 0.2717],\n",
      "         [0.4763, 0.9509]],\n",
      "\n",
      "        [[0.1758, 0.1981],\n",
      "         [0.6707, 0.6077],\n",
      "         [0.7615, 0.3354]],\n",
      "\n",
      "        [[0.6914, 0.6997],\n",
      "         [0.6894, 0.2121],\n",
      "         [0.2538, 0.6794]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(batch.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8fa852d-b326-4312-a767-45b403b9b8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(batch.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c7f02-b8d8-488b-8b73-14c19d30c05f",
   "metadata": {},
   "source": [
    "### Quick explanation of torch.repeat or np.tiles: https://www.sharpsightlabs.com/blog/numpy-tile/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a6c2230-2324-43dc-b62a-83aa3d67a43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 6])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_sentence.T.repeat(3, 1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4977654f-2f7f-4c01-9115-a81cfff7a681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_sentence.T.repeat(3, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf89a2-8b85-43db-b887-4c3e74fb92a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
